{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\miniconda3\\envs\\rag-chat\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from time import time\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"context\",\n",
    "    memory = memory,\n",
    "    system_prompt = (\n",
    "        \"You are my helpful assitant\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the article, as of January 2023, the ratio of logged to registered models is approximately 2.9:1. This means that for every 2.9 experimental models (logged), about 1 model is registered and considered a candidate for production.\n",
      "Elapsed time 30.31207275390625s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "response = chat_engine.chat(\"From the databricks article, what is the ratio of logged to registered models? \")\n",
    "t1 = time()\n",
    "\n",
    "print(response)\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article doesn't explicitly mention what it means by \"logged\" and \"registered\" models.\n",
      "\n",
      "However, based on the context, I can make an educated guess. In the context of machine learning and data science, being \"logged\" might mean that a model has been created, trained, or experimented with (i.e., logged into some kind of system), but not necessarily intended for production use. On the other hand, being \"registered\" might imply that a model has met certain criteria, such as quality standards, and is deemed suitable for deployment in a production environment.\n",
      "\n",
      "If I had to hazard a guess based on this context, it's possible that logged models are essentially experimental or prototype models, whereas registered models are those that have been vetted and validated for use in actual applications.\n",
      "Elapsed time 36.605592250823975s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "response = chat_engine.chat(\"What does that means?\")\n",
    "t1 = time()\n",
    "\n",
    "print(response)\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd say something like:\n",
      "\n",
      "\"In the context of this report, 'logged' models refer to experiments or prototypes that have been created and tried out. On the other hand, 'registered' models are those that have met certain standards and are ready for use in real-world applications.\n",
      "\n",
      "Think of it like a car test drive: logged models would be like taking a car for a spin to see how it handles, while registered models would be like buying a car that's been thoroughly inspected and certified to be road-worthy.\"\n",
      "\n",
      "This analogy should give business stakeholders an idea of the difference between these two concepts without requiring technical expertise!\n",
      "Elapsed time 42.56543779373169s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "response = chat_engine.chat(\"If you were to explain that to business stakeholder with limited technical knowledge how would you rephrase it?\")\n",
    "t1 = time()\n",
    "\n",
    "print(response)\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='According to the article, as of January 2023, the ratio of logged to registered models is approximately 2.9:1. This means that for every 2.9 experimental models (logged), about 1 model is registered and considered a candidate for production.', additional_kwargs={'tool_calls': []})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_store.store['chat_history'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ChatMemoryBuffer.to_string of ChatMemoryBuffer(chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, content='From the databricks article, what is the ratio of logged to registered models? ', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='According to the article, as of January 2023, the ratio of logged to registered models is approximately 2.9:1. This means that for every 2.9 experimental models (logged), about 1 model is registered and considered a candidate for production.', additional_kwargs={'tool_calls': []}), ChatMessage(role=<MessageRole.USER: 'user'>, content='What does that means?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The article doesn\\'t explicitly mention what it means by \"logged\" and \"registered\" models.\\n\\nHowever, based on the context, I can make an educated guess. In the context of machine learning and data science, being \"logged\" might mean that a model has been created, trained, or experimented with (i.e., logged into some kind of system), but not necessarily intended for production use. On the other hand, being \"registered\" might imply that a model has met certain criteria, such as quality standards, and is deemed suitable for deployment in a production environment.\\n\\nIf I had to hazard a guess based on this context, it\\'s possible that logged models are essentially experimental or prototype models, whereas registered models are those that have been vetted and validated for use in actual applications.', additional_kwargs={'tool_calls': []}), ChatMessage(role=<MessageRole.USER: 'user'>, content='If you were to explain that to business stakeholder with limited technical knowledge how would you rephrase it?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='I\\'d say something like:\\n\\n\"In the context of this report, \\'logged\\' models refer to experiments or prototypes that have been created and tried out. On the other hand, \\'registered\\' models are those that have met certain standards and are ready for use in real-world applications.\\n\\nThink of it like a car test drive: logged models would be like taking a car for a spin to see how it handles, while registered models would be like buying a car that\\'s been thoroughly inspected and certified to be road-worthy.\"\\n\\nThis analogy should give business stakeholders an idea of the difference between these two concepts without requiring technical expertise!', additional_kwargs={'tool_calls': []})]}), chat_store_key='chat_history', token_limit=1500, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='From the databricks article, what is the ratio of logged to registered models? ', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='According to the article, as of January 2023, the ratio of logged to registered models is approximately 2.9:1. This means that for every 2.9 experimental models (logged), about 1 model is registered and considered a candidate for production.', additional_kwargs={'tool_calls': []}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='What does that means?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The article doesn\\'t explicitly mention what it means by \"logged\" and \"registered\" models.\\n\\nHowever, based on the context, I can make an educated guess. In the context of machine learning and data science, being \"logged\" might mean that a model has been created, trained, or experimented with (i.e., logged into some kind of system), but not necessarily intended for production use. On the other hand, being \"registered\" might imply that a model has met certain criteria, such as quality standards, and is deemed suitable for deployment in a production environment.\\n\\nIf I had to hazard a guess based on this context, it\\'s possible that logged models are essentially experimental or prototype models, whereas registered models are those that have been vetted and validated for use in actual applications.', additional_kwargs={'tool_calls': []}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='If you were to explain that to business stakeholder with limited technical knowledge how would you rephrase it?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='I\\'d say something like:\\n\\n\"In the context of this report, \\'logged\\' models refer to experiments or prototypes that have been created and tried out. On the other hand, \\'registered\\' models are those that have met certain standards and are ready for use in real-world applications.\\n\\nThink of it like a car test drive: logged models would be like taking a car for a spin to see how it handles, while registered models would be like buying a car that\\'s been thoroughly inspected and certified to be road-worthy.\"\\n\\nThis analogy should give business stakeholders an idea of the difference between these two concepts without requiring technical expertise!', additional_kwargs={'tool_calls': []})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the conversation, it appears that Superconductive is one of the emerging challenger tools in the data integration category, along with Great Expectations as a specific product, although it didn't explicitly state that Superconductive is an emerging challenger tool.\n",
      "Elapsed time 343.28652358055115s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "response = chat_engine.chat(\"Which are the fastest growing Data and AI products?\")\n",
    "t1 = time()\n",
    "\n",
    "print(response)\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the parent directory (my_project) to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\miniconda3\\envs\\rag-chat\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no mention of the author's childhood or upbringing in the provided context. The text appears to be a report on the state of data and AI, with discussions on language models, LLMs, and their applications. Therefore, it is not possible to answer the query about what the author did growing up based on this information.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from time import time\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0)\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks\\..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf\n",
      "c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks\\..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf\n"
     ]
    }
   ],
   "source": [
    "for doc in response.source_nodes:\n",
    "    print(doc.node.extra_info['file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='fc1a5aab-59cf-41ea-b174-6546d9a3f492', embedding=None, metadata={'page_label': '12', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': 'c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks\\\\..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='eadc58e2-a69f-44fd-8cbb-53b07ccf5d98', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': 'c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks\\\\..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, hash='de4b98d5907e9878b7bb5ad7f170d451c01ad5f148bf875f1ff2f595a2ccf0be')}, text='STATE OF DATA + AI12\\nLarge language models are  \\nthe “it” tool\\nLLMs are currently one of the hottest and most-watched areas \\nin the field of NLP. LLMs have been instrumental in enabling \\nmachines to understand, interpret and generate human \\nlanguage in a way that was previously impossible, powering \\neverything from machine translation to content creation to \\nvirtual assistants and chatbots.\\nTransformer-related libraries had been growing in popularity \\neven before ChatGPT thrust LLMs into the public consciousness. \\nWithin the last 6 months, our data shows two accelerating \\ntrends: organizations are building their own LLMs, which models \\nlike Dolly  show can be quite accessible and inexpensive. And, \\nthey are using proprietary models like ChatGPT. Transformer-\\nrelated libraries, such as Hugging Face, which are used to train \\nLLMs, have the highest adoption within the Lakehouse. \\nThe second most popular type is SaaS LLMs, which are used  \\nto access models like OpenAI. This category has grown \\nexponentially in parallel with the launch of ChatGPT : the  \\nnumber of Lakehouse customers using SaaS LLMs grew  \\nan impressive 1310% between the end of November 2022 and \\nthe beginning of May 2023. (In contrast, transformer-related  \\nlibraries grew 82% in this same period.) Organizations can leverage LLMs either by \\nusing SaaS LLM APIs to call services like \\nChatGPT from OpenAI or by operating their \\nown LLMs in-house.\\nThinking of building your own modern LLM \\napplication? This approach could entail \\nthe use of specialized transformer-related \\nPython libraries to train the model, as well as \\nLLM tools like LangChain to develop prompt \\ninterfaces or integrations to other systems.  \\nLLM DEFINITIONS\\n◊\\t Transformer-related libraries:  \\nPython libraries used to train LLMs \\n(example: Hugging Face)\\n◊\\t SaaS LLM APIs: Libraries used to access \\nLLMs as a service (example: OpenAI)\\n◊\\t LLM tools:  Toolchains for working  \\nwith and building proprietary LLMs \\n(example: LangChain)', mimetype='text/plain', start_char_idx=0, end_char_idx=1985, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3442480843339226)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Memory Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from time import time\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"context\",\n",
    "    memory = memory,\n",
    "    system_prompt = (\n",
    "        \"You are my helpful assitant\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me check the document at `c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\n",
      "\n",
      "According to my analysis, the document contains two articles:\n",
      "\n",
      "1. \"STATE OF DATA + AI\" (Page 1)\n",
      "2. \"STATE OF DATA + AI2\" (Page 2)\n",
      "Elapsed time 25.415090322494507s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "response = chat_engine.chat(\"What articles do I have in the docstore? \")\n",
    "t1 = time()\n",
    "\n",
    "print(response)\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let me check the document at `c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\n",
      "\n",
      "According to my analysis, the document contains two articles:\n",
      "\n",
      "1. \"STATE OF DATA + AI\" (Page 1)\n",
      "2. \"STATE OF DATA + AI2\" (Page 2)\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_store': {'store': {'chat_history': [{'role': <MessageRole.USER: 'user'>,\n",
       "     'content': 'What articles do I have in the docstore? ',\n",
       "     'additional_kwargs': {}},\n",
       "    {'role': <MessageRole.ASSISTANT: 'assistant'>,\n",
       "     'content': 'Let me check the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\\n\\nAccording to my analysis, the document contains two articles:\\n\\n1. \"STATE OF DATA + AI\" (Page 1)\\n2. \"STATE OF DATA + AI2\" (Page 2)',\n",
       "     'additional_kwargs': {'tool_calls': []}}]},\n",
       "  'class_name': 'SimpleChatStore'},\n",
       " 'chat_store_key': 'chat_history',\n",
       " 'token_limit': 1500,\n",
       " 'class_name': 'ChatMemoryBuffer'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_dict = memory.to_dict()\n",
    "memory_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me check the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\\n\\nAccording to my analysis, the document contains two articles:\\n\\n1. \"STATE OF DATA + AI\" (Page 1)\\n2. \"STATE OF DATA + AI2\" (Page 2)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_dict['chat_store']['store']['chat_history'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_memory = ChatMemoryBuffer.from_dict(memory_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on my knowledge and the context of the document, here are some best AI practices that might be relevant:\n",
      "\n",
      "1. **Use specialized libraries**: When working with NLP tasks, use specialized libraries like NLTK, Transformers, or FuzzyWuzzy to get the most out of your data.\n",
      "2. **Leverage domain expertise**: Apply AI and ML techniques to specific industries like Retail and CPG for time series forecasting, or utilize domain-specific knowledge when building chatbots.\n",
      "3. **Monitor and iterate**: Continuously monitor the performance of your AI models and iteratively improve them based on feedback from users.\n",
      "4. **Transparency and explainability**: Provide transparent and explainable results to build trust with stakeholders and users.\n",
      "5. **Human oversight and review**: Implement human oversight and review processes for critical decisions made by AI systems.\n",
      "\n",
      "Please note that these best practices are general guidelines, and the specific context of your project might require additional considerations.\n",
      "\n",
      "Would you like me to elaborate on any of these points?Elapsed time 53.540040254592896s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "streaming_response = chat_engine.stream_chat(\"Best AI practices\")\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\")\n",
    "t1 = time()\n",
    "\n",
    "print(f\"Elapsed time {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: user, Content: What articles do I have in the docstore? \n",
      "Role: assistant, Content: Let me check the document at `c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\n",
      "\n",
      "According to my analysis, the document contains two articles:\n",
      "\n",
      "1. \"STATE OF DATA + AI\" (Page 1)\n",
      "2. \"STATE OF DATA + AI2\" (Page 2)\n",
      "Role: user, Content: Best AI practices\n",
      "Role: assistant, Content: Based on the document at `c:\\Users\\Carlos\\Desktop\\RAG-chatbot\\code project\\notebooks..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf`, I found that one of the most popular data science use cases is Natural Language Processing (NLP), which accounted for 49% of all libraries used.\n",
      "\n",
      "Some best AI practices mentioned in the document include:\n",
      "\n",
      "* Using specialized Python libraries such as NLTK, Transformers, and FuzzyWuzzy to leverage NLP capabilities\n",
      "* Applying NLP to use cases like chatbots, research assistance, fraud detection, content generation, and more\n",
      "* Utilizing data integration tools like dbt to support advanced ML and AI use cases\n",
      "\n",
      "However, I couldn't find any specific \"best practices\" section in the document. If you're looking for more general AI best practices, I'd be happy to provide some general guidelines!\n",
      "Role: user, Content: Best AI practices\n",
      "Role: assistant, Content: Based on my knowledge and the context of the document, here are some best AI practices that might be relevant:\n",
      "\n",
      "1. **Use specialized libraries**: When working with NLP tasks, use specialized libraries like NLTK, Transformers, or FuzzyWuzzy to get the most out of your data.\n",
      "2. **Leverage domain expertise**: Apply AI and ML techniques to specific industries like Retail and CPG for time series forecasting, or utilize domain-specific knowledge when building chatbots.\n",
      "3. **Monitor and iterate**: Continuously monitor the performance of your AI models and iteratively improve them based on feedback from users.\n",
      "4. **Transparency and explainability**: Provide transparent and explainable results to build trust with stakeholders and users.\n",
      "5. **Human oversight and review**: Implement human oversight and review processes for critical decisions made by AI systems.\n",
      "\n",
      "Please note that these best practices are general guidelines, and the specific context of your project might require additional considerations.\n",
      "\n",
      "Would you like me to elaborate on any of these points?\n"
     ]
    }
   ],
   "source": [
    "for message in memory.get_all():\n",
    "    print(f\"Role: {message.role}, Content: {message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "history =[]\n",
    "for message in memory.get_all():\n",
    "    history.append({'role':message.role.value,'content':message.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What articles do I have in the docstore? '},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Let me check the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\\n\\nAccording to my analysis, the document contains two articles:\\n\\n1. \"STATE OF DATA + AI\" (Page 1)\\n2. \"STATE OF DATA + AI2\" (Page 2)'},\n",
       " {'role': 'user', 'content': 'Best AI practices'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf`, I found that one of the most popular data science use cases is Natural Language Processing (NLP), which accounted for 49% of all libraries used.\\n\\nSome best AI practices mentioned in the document include:\\n\\n* Using specialized Python libraries such as NLTK, Transformers, and FuzzyWuzzy to leverage NLP capabilities\\n* Applying NLP to use cases like chatbots, research assistance, fraud detection, content generation, and more\\n* Utilizing data integration tools like dbt to support advanced ML and AI use cases\\n\\nHowever, I couldn\\'t find any specific \"best practices\" section in the document. If you\\'re looking for more general AI best practices, I\\'d be happy to provide some general guidelines!'},\n",
       " {'role': 'user', 'content': 'Best AI practices'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Based on my knowledge and the context of the document, here are some best AI practices that might be relevant:\\n\\n1. **Use specialized libraries**: When working with NLP tasks, use specialized libraries like NLTK, Transformers, or FuzzyWuzzy to get the most out of your data.\\n2. **Leverage domain expertise**: Apply AI and ML techniques to specific industries like Retail and CPG for time series forecasting, or utilize domain-specific knowledge when building chatbots.\\n3. **Monitor and iterate**: Continuously monitor the performance of your AI models and iteratively improve them based on feedback from users.\\n4. **Transparency and explainability**: Provide transparent and explainable results to build trust with stakeholders and users.\\n5. **Human oversight and review**: Implement human oversight and review processes for critical decisions made by AI systems.\\n\\nPlease note that these best practices are general guidelines, and the specific context of your project might require additional considerations.\\n\\nWould you like me to elaborate on any of these points?'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n"
     ]
    }
   ],
   "source": [
    "print(message.role.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatMemoryBuffer(chat_store=SimpleChatStore(store={'chat_history': [ChatMessage(role=<MessageRole.USER: 'user'>, content='What articles do I have in the docstore? ', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Let me check the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\\n\\nAccording to my analysis, the document contains two articles:\\n\\n1. \"STATE OF DATA + AI\" (Page 1)\\n2. \"STATE OF DATA + AI2\" (Page 2)', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Best AI practices', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf`, I found that one of the most popular data science use cases is Natural Language Processing (NLP), which accounted for 49% of all libraries used.\\n\\nSome best AI practices mentioned in the document include:\\n\\n* Using specialized Python libraries such as NLTK, Transformers, and FuzzyWuzzy to leverage NLP capabilities\\n* Applying NLP to use cases like chatbots, research assistance, fraud detection, content generation, and more\\n* Utilizing data integration tools like dbt to support advanced ML and AI use cases\\n\\nHowever, I couldn\\'t find any specific \"best practices\" section in the document. If you\\'re looking for more general AI best practices, I\\'d be happy to provide some general guidelines!', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Best AI practices', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on my knowledge and the context of the document, here are some best AI practices that might be relevant:\\n\\n1. **Use specialized libraries**: When working with NLP tasks, use specialized libraries like NLTK, Transformers, or FuzzyWuzzy to get the most out of your data.\\n2. **Leverage domain expertise**: Apply AI and ML techniques to specific industries like Retail and CPG for time series forecasting, or utilize domain-specific knowledge when building chatbots.\\n3. **Monitor and iterate**: Continuously monitor the performance of your AI models and iteratively improve them based on feedback from users.\\n4. **Transparency and explainability**: Provide transparent and explainable results to build trust with stakeholders and users.\\n5. **Human oversight and review**: Implement human oversight and review processes for critical decisions made by AI systems.\\n\\nPlease note that these best practices are general guidelines, and the specific context of your project might require additional considerations.\\n\\nWould you like me to elaborate on any of these points?', additional_kwargs={})]}), chat_store_key='chat_history', token_limit=1500, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mem = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "for message in history:\n",
    "    buff = ChatMessage(role=message['role'],content=message['content'])\n",
    "    new_mem.put(buff)\n",
    "new_mem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What articles do I have in the docstore? ', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Let me check the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf` for you.\\n\\nAccording to my analysis, the document contains two articles:\\n\\n1. \"STATE OF DATA + AI\" (Page 1)\\n2. \"STATE OF DATA + AI2\" (Page 2)', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Best AI practices', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on the document at `c:\\\\Users\\\\Carlos\\\\Desktop\\\\RAG-chatbot\\\\code project\\\\notebooks..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf`, I found that one of the most popular data science use cases is Natural Language Processing (NLP), which accounted for 49% of all libraries used.\\n\\nSome best AI practices mentioned in the document include:\\n\\n* Using specialized Python libraries such as NLTK, Transformers, and FuzzyWuzzy to leverage NLP capabilities\\n* Applying NLP to use cases like chatbots, research assistance, fraud detection, content generation, and more\\n* Utilizing data integration tools like dbt to support advanced ML and AI use cases\\n\\nHowever, I couldn\\'t find any specific \"best practices\" section in the document. If you\\'re looking for more general AI best practices, I\\'d be happy to provide some general guidelines!', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Best AI practices', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Based on my knowledge and the context of the document, here are some best AI practices that might be relevant:\\n\\n1. **Use specialized libraries**: When working with NLP tasks, use specialized libraries like NLTK, Transformers, or FuzzyWuzzy to get the most out of your data.\\n2. **Leverage domain expertise**: Apply AI and ML techniques to specific industries like Retail and CPG for time series forecasting, or utilize domain-specific knowledge when building chatbots.\\n3. **Monitor and iterate**: Continuously monitor the performance of your AI models and iteratively improve them based on feedback from users.\\n4. **Transparency and explainability**: Provide transparent and explainable results to build trust with stakeholders and users.\\n5. **Human oversight and review**: Implement human oversight and review processes for critical decisions made by AI systems.\\n\\nPlease note that these best practices are general guidelines, and the specific context of your project might require additional considerations.\\n\\nWould you like me to elaborate on any of these points?', additional_kwargs={})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mem.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the paper is discussing a \"State of Data\" report, but it doesn't specify which type of AI (e.g. Machine Learning, Deep Learning, Natural Language Processing, etc.) is preferred.\n",
      "\n",
      "However, I can suggest that you look for keywords such as \"Machine Learning\", \"Deep Learning\", \"NLP\", or \"Computer Vision\" in the paper to get a better understanding of the types of AI mentioned. If you provide me with more context or information about the content of the paper, I'd be happy to try and assist you further!\n",
      "Source documents:\n",
      "- ..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "\n",
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.1\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"context\",\n",
    "    memory = memory,\n",
    "    system_prompt = (\n",
    "        \"You are my helpful assitant\"\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = chat_engine.stream_chat(\"Which AI is preffered in the paper\")\n",
    "\n",
    "for token in stream.response_gen:\n",
    "    print(token, end=\"\")\n",
    "\n",
    "\n",
    "def get_relative_source_file_paths(response):\n",
    "    cwd = os.getcwd()\n",
    "    source_nodes = response.source_nodes\n",
    "    file_paths = set()\n",
    "    for node in source_nodes:\n",
    "        if 'file_path' in node.metadata:\n",
    "            # Convert absolute path to relative path\n",
    "            rel_path = os.path.relpath(node.metadata['file_path'], cwd)\n",
    "            file_paths.add(rel_path)\n",
    "    return list(file_paths)\n",
    "\n",
    "source_files = get_relative_source_file_paths(stream)\n",
    "if source_files:\n",
    "    print(\"\\nSource documents:\")\n",
    "    for file in source_files:\n",
    "        print(f\"- {file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fp = documents[0].metadata['file_path']\n",
    "cwd = os.getcwd()\n",
    "rel_path = os.path.relpath(fp, cwd)\n",
    "rel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    fp = doc.metadata['file_path']\n",
    "    doc.metadata['file_path'] = os.path.relpath(fp, cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='ede2e5c7-12e0-475a-a8ca-a1da05e522b8', embedding=None, metadata={'page_label': '1', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI1\\n State of  \\nData + AI', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='de5a08d1-24e9-43ed-9346-49b0dfa4f766', embedding=None, metadata={'page_label': '2', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI2\\nWe’re in the \\ngolden age of \\ndata and AI \\nSTATE OF DATA + AI 2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4d6dcbed-a8d4-4441-b793-7d3c2ec2b7e5', embedding=None, metadata={'page_label': '3', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI3\\nIn the 6 months since ChatGPT launched, the world has woken up to the vast potential  \\nof AI. The unparalleled pace of AI discoveries, model improvements and new products  \\non the market puts data and AI strategy at the top of conversations across every \\norganization around the world. We believe that AI will usher in the next generation of \\nproduct and software innovation, and we’re already seeing this play out in the market.  \\nThe next generation of winning companies and executives will be those who understand \\nand leverage AI.  \\nIn this report, we examine patterns and trends in data and AI adoption across more \\nthan 9,000 global Databricks customers. By unifying business intelligence (BI) and AI \\napplications across companies’ entire data estates, the Databricks Lakehouse provides \\na unique vantage point into the state of data and AI, including which products and \\ntechnologies are the fastest growing, the types of data science and machine learning  \\n(DS/ML) applications being developed and more. INTRO\\nSTATE OF DATA + AI 3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f3e119e2-7ea2-4c91-a566-6ac062766756', embedding=None, metadata={'page_label': '4', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='We hope that by sharing these trends, data leaders will be able to benchmark \\ntheir organizations and gain insights that help inform their strategies for an  \\nera defined by data and AI.Here are the major stories we uncovered:\\nSTATE OF DATA + AI 4Companies are adopting \\nmachine learning and large \\nlanguage models (LLMs)  \\nat a rapid pace. Natural \\nlanguage processing (NLP)  \\nis dominating use cases,  \\nwith an accelerated focus  \\non LLMs.Open source wins in today’s \\ndata and AI markets. Eight out  \\nof 10 of our most widely \\nadopted AI and machine \\nlearning products are based  \\non open source.Organizations are increasingly \\nusing the Lakehouse for data \\nwarehousing, as evidenced \\nby the high growth of data \\nintegration tools dbt and \\nFivetran, and the accelerated \\nadoption of Databricks SQL.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2b75766c-4f94-4ec8-8f1f-13534283e197', embedding=None, metadata={'page_label': '5', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI5\\n• The number of companies using SaaS LLM APIs (used to access \\nservices like ChatGPT) grew 1310% between the end of November \\n2022 and the beginning of May 2023\\n• NLP accounts for 49% of daily Python data science library usage, \\nmaking it the most popular application\\n• Organizations are putting substantially more models into \\nproduction (411% YoY growth) while also increasing their ML \\nexperimentation (54% YoY growth)\\n• Organizations are getting more efficient with ML; for every three \\nexperimental models, roughly one is put into production, compared \\nto five experimental models a year priorSummary of  \\nKey Findings\\n1DATA SCIENCE AND MACHINE LEARNING:  \\nNLP AND LLMS ARE IN HIGH DEMAND', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='977c1175-1060-4997-a386-7d96355ad5bd', embedding=None, metadata={'page_label': '6', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI6\\n• BI is the top data and AI market, but \\ngrowth trends in other markets show that \\ncompanies are increasingly looking at \\nmore advanced data use cases\\n• The fastest-growing data and AI product \\nis dbt, which grew 206% YoY by number \\nof customers\\n• Data integration is the fastest-growing \\ndata and AI market on the Databricks \\nLakehouse with 117% YoY growth• 61% of customers migrating to the \\nLakehouse are coming from on-\\nprem and cloud data warehouses\\n• The volume of data in Delta Lake \\nhas grown 304% YoY\\n• The Lakehouse is increasingly \\nbeing used for data warehousing, \\nincluding serverless data \\nwarehousing with Databricks  \\nSQL, which grew 144% YoY2FASTEST-GROWING DATA \\nAND AI PRODUCTS\\n3ADOPTION AND \\nMIGRATION TRENDS ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5d8bbb40-c6fc-47fd-83e7-457174cf8198', embedding=None, metadata={'page_label': '7', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI7\\nMethodology: How did Databricks  \\ncreate this report?\\nThe State of Data + AI is built from fully aggregated, anonymized data collected \\nfrom our customers based on how they are using the Databricks Lakehouse  \\nand its broad ecosystem of integrated tools. This report focuses on machine \\nlearning adoption, data architecture (integrations and migrations) and use cases.  \\nThe customers in this report represent every major industry and range in size  \\nfrom startups to many of the world’s largest enterprises.  \\nUnless otherwise noted, this report presents and analyzes data from February 1, \\n2022, to January 31, 2023, and usage is measured by number of customers.  \\nWhen possible, we provide YoY comparisons to showcase growth trends over time.\\nSTATE OF DATA + AI 7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c4bb91d8-0cb3-46bb-9c89-a8ce5cc25f80', embedding=None, metadata={'page_label': '8', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI8\\nData Science and  \\nMachine Learning \\nNATURAL LANGUAGE PROCESSING AND LARGE \\nLANGUAGE MODELS ARE IN HIGH DEMAND\\nAcross all industries, companies leverage data science and \\nmachine learning (DS/ML) to accelerate growth, improve \\npredictability and enhance customer experiences. Recent \\nadvancements in large language models (LLMs) are propelling \\ncompanies to rethink AI within their own data strategies.  \\nGiven the rapidly evolving DS/ML landscape, we wanted to \\nunderstand several aspects of the market: \\n \\n• Which types of DS/ML applications are companies investing \\nin? In particular, given the recent buzz, what does the data \\naround LLMs look like?\\n• Are companies making headway on operationalizing  \\ntheir machine learning models (MLOps)?\\nSTATE OF DATA + AI 8', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f44b2476-0709-4d92-a514-df4e11603e18', embedding=None, metadata={'page_label': '9', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI9\\nNote: This chart reflects the unique \\nnumber of notebooks using ML \\nlibraries per day in each of the \\ncategories. It includes libraries used \\nfor the particular problem-solving use \\ncases mentioned. It does not include \\nlibraries used in tooling for data \\npreparations and modeling.  SPECIALIZED PYTHON  DS/ML  \\n LIBRARIES FROM  FEBRUARY 2022   \\n TO JANUARY 2023 Time SeriesDS/ML ApplicationsTime Series\\nSpeech Recognition\\nRecommender Systems\\nIndustry Data Modeling\\nGraph\\nGeospatial\\nComputer Vision\\nAnomaly Detection \\n & SegmentationSimulations &    \\nOptimizations\\nNatural  \\n Language   \\nProcessing', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cd314071-e1fc-4dee-9f69-ec0eb158d72e', embedding=None, metadata={'page_label': '10', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI10\\nOur second most popular DS/ML application is \\nsimulations and optimization, which accounts for 30% of \\nall use cases. This signals organizations are using data to \\nmodel prototypes and solve problems cost-effectively. \\nMany of the DS/ML use cases are predominantly \\nleveraged by specific industries. While they take up a \\nsmaller share of the total, they are mission-critical for \\nmany organizations. For example, time series includes \\nforecasting, a use case that is especially popular in \\nindustries such as Retail and CPG, which rely heavily  \\non the ability to forecast the need for every item in  \\nevery store. Natural language processing dominates  \\nmachine learning use cases\\nTo understand how organizations are applying AI and \\nML within the Lakehouse, we aggregated the usage \\nof specialized Python libraries, which include NLTK, \\nTransformers and FuzzyWuzzy, into popular data science \\nuse cases.1 We look at data from these libraries because \\nPython is on the cutting edge of new developments in ML, \\nadvanced analytics and AI, and has consistently ranked  \\nas one of the most popular programming languages  in \\nrecent years.\\nOur most popular use case is natural language processing \\n(NLP), a rapidly growing field that enables businesses to \\ngain value from unstructured textual data. This opens the \\ndoor for users to accomplish tasks that were previously \\ntoo abstract for code, such as summarizing content \\nor extracting sentiment from customer reviews. In our \\ndata set, 49% of libraries used are associated with NLP. \\nLLMs also fall within this bucket. Given the innovations \\nlaunched in recent months, we expect to see NLP take \\noff even more in coming years as it is applied to use \\ncases like chatbots, research assistance, fraud detection, \\ncontent generation and more. In our data set, 49% of \\nspecialized Python libraries \\nused are associated with NLP\\n1. This data does not include general-purpose ML libraries, including \\nscikit-learn or TensorFlow.\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2e1ce06f-2322-440a-9c4f-103d54886044', embedding=None, metadata={'page_label': '11', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI11\\nNote: There are several popular types of Python libraries that are commonly used for LLMs.  \\nThese libraries provide pretrained models and tools for building, training and deploying LLMs.  \\nWe have rolled these libraries up into groupings based on the type of functionality they provide.\\nData consistently dips in the last week of December due to seasonality. USE OF LARGE LANGUAGE MODELS (LLMS) \\n \\nFeb \\n2022Feb Mar Mar Apr Apr May May June July Aug Sept Oct Nov Dec Jan \\n2023Number of CustomersTransformer-Related \\n Libraries\\nSaaS LLM APIs\\nLLM ToolsNov 30, 2022  \\n ChatGPT LaunchMarch 24, 2023 \\nDolly Launch', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e00f6ea0-23ed-4d66-991f-521588837dfd', embedding=None, metadata={'page_label': '12', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI12\\nLarge language models are  \\nthe “it” tool\\nLLMs are currently one of the hottest and most-watched areas \\nin the field of NLP. LLMs have been instrumental in enabling \\nmachines to understand, interpret and generate human \\nlanguage in a way that was previously impossible, powering \\neverything from machine translation to content creation to \\nvirtual assistants and chatbots.\\nTransformer-related libraries had been growing in popularity \\neven before ChatGPT thrust LLMs into the public consciousness. \\nWithin the last 6 months, our data shows two accelerating \\ntrends: organizations are building their own LLMs, which models \\nlike Dolly  show can be quite accessible and inexpensive. And, \\nthey are using proprietary models like ChatGPT. Transformer-\\nrelated libraries, such as Hugging Face, which are used to train \\nLLMs, have the highest adoption within the Lakehouse. \\nThe second most popular type is SaaS LLMs, which are used  \\nto access models like OpenAI. This category has grown \\nexponentially in parallel with the launch of ChatGPT : the  \\nnumber of Lakehouse customers using SaaS LLMs grew  \\nan impressive 1310% between the end of November 2022 and \\nthe beginning of May 2023. (In contrast, transformer-related  \\nlibraries grew 82% in this same period.) Organizations can leverage LLMs either by \\nusing SaaS LLM APIs to call services like \\nChatGPT from OpenAI or by operating their \\nown LLMs in-house.\\nThinking of building your own modern LLM \\napplication? This approach could entail \\nthe use of specialized transformer-related \\nPython libraries to train the model, as well as \\nLLM tools like LangChain to develop prompt \\ninterfaces or integrations to other systems.  \\nLLM DEFINITIONS\\n◊\\t Transformer-related libraries:  \\nPython libraries used to train LLMs \\n(example: Hugging Face)\\n◊\\t SaaS LLM APIs: Libraries used to access \\nLLMs as a service (example: OpenAI)\\n◊\\t LLM tools:  Toolchains for working  \\nwith and building proprietary LLMs \\n(example: LangChain)', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8db9854e-34b2-404c-9522-15e82b2e4692', embedding=None, metadata={'page_label': '13', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI13\\nMachine learning experimentation and production  \\ntake off across industries\\nThe increasing demand for ML solutions and the growing \\navailability of technologies have led to a significant \\nincrease in experimentation and production, two distinct \\nparts of the ML model lifecycle. We look at the logging  \\nand registering  of models in MLflow, an open source \\nplatform developed by Databricks, to understand how ML \\nis trending and being adopted within organizations. MLflow Model Registry launched in May 2021. Overall, the \\nnumber of logged models has grown 54% since February \\n2022, while the number of registered models has grown \\n411% over the same period. This growth in volume \\nsuggests organizations are understanding the value of \\ninvesting in and allocating more people power to ML.\\nLOGGED MODELS AND  \\nML EXPERIMENTATION\\nDuring the experimentation phase of ML, data scientists \\ndevelop models designed to solve given tasks. After training \\nthe models, they test them to evaluate their accuracy, \\nprecision, recall (the percentage of correctly predicted \\npositive instances out of all actual positive instances), \\nand more. These metrics are logged (recorded) in order to \\nanalyze the various models’ performance and identify which \\napproach works best for the given task. \\nWe have chosen logged models as a proxy to measure ML \\nexperimentation because the MLflow Tracking Server is \\ndesigned to facilitate experiment tracking and reproducibility. REGISTERED MODELS AND ML PRODUCTION\\nProduction models have undergone the experimentation \\nphase and are then deployed in real-world applications. They \\nare typically used to make predictions or decisions based on \\nnew data. Registering a model is the process of recording and \\nstoring metadata about a trained model in a centralized location \\nthat allows users to easily access and reuse existing models. \\nRegistering models prior to production enables organizations to \\nensure consistency and reliability in model deployment and scale. \\nWe have chosen registered models to represent ML production \\nbecause the MLflow Model Registry is designed to manage \\nmodels that have left the experimentation phase through the  \\nrest of their lifecycle.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='bf9cff05-5bd9-433c-9bbc-d613f33002df', embedding=None, metadata={'page_label': '14', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI14\\nOrganizations test numerous approaches and variables \\nbefore committing an ML model to production. We \\nwanted to understand, “How many models do data \\nscientists experiment with before moving to production?”\\nOur data shows the ratio of logged to registered models \\nis 2.9 : 1 as of January 2023. This means that for roughly \\nevery three experimental models, one model will get \\nregistered as a candidate for production. This ratio has \\nimproved significantly from just a year prior, when we \\n2.9 : 1 RATIO OF LOGGED VS.  \\n REGISTERED MODELS \\nNumber of Models\\nRatio of Logged to Registered  \\nModels in Jan 2023Feb \\n2022Mar Apr May June July Aug Sept Oct Nov Dec Jan \\n2023saw that for roughly every five experimental models, one \\nwas registered. Recent advances in ML, such as improved \\nopen source libraries like MLflow and Hugging Face, have \\nradically simplified building and putting models into \\nproduction. The result is that 34% of logged models are \\nnow candidates for production today, an improvement \\nfrom over 20% just a year ago. ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e3a94597-4f58-49de-abf2-461345795a78', embedding=None, metadata={'page_label': '15', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI15\\nThe Modern Data \\nand AI Stack\\nOver the last several years, the trend toward building \\nopen, unified data architectures has played out in our \\nown data. We see that data leaders are opting to preserve \\nchoice, leverage the best products and deliver innovation \\nacross their organizations by democratizing access to \\ndata for more people.\\nSTATE OF DATA + AI 15', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='e84576b3-71b7-420f-a6ce-9c398724dd83', embedding=None, metadata={'page_label': '16', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI16\\n STATE OF DATA + AI 16    TOP 5 AI AND MACHINE LEARNING PRODUCTS \\nFeb \\n2022Mar Apr May June July Aug Sept Oct Nov Dec Jan \\n2023Number of Customers\\nFeb\\n2022Mar Apr May June July Aug Sept Oct Nov Dec Jan\\n2023Number of CustomersPlotly and Dash\\nHugging Face\\nJohn Snow Labs\\nLabelbox\\nNVIDIA', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b1d4226e-2360-40cc-a498-31c6e02b1124', embedding=None, metadata={'page_label': '17', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI17\\nAs companies integrate data science and ML into \\ntheir business strategies, many leaders are looking for \\nguidance on the right tools to add to their arsenals. The \\nDatabricks Lakehouse integrates with a growing number \\nof AI and ML solutions to support these use cases.\\nOne of our most interesting findings is that open source \\nis dominating the top ranks; 3 out of our 5 most widely \\nadopted AI and ML products on the Lakehouse are based \\non open source. This indicates a growing sentiment across \\nindustries: open platforms and products are critical to \\ntoday’s AI and ML strategies. We anticipate this trend to \\ncontinue with the rise of generative AI. Many organizations \\nwant to leverage LLMs but share concerns over issues \\nsuch as data privacy for their sensitive data. Open source \\nand open models empower organizations to build LLMs \\nwithout relying on third-party proprietary tools.Top AI and ML Products\\nLaunched in October 2022, LangChain is an \\nopen source framework for developing LLM \\napplications. As a new integration, LangChain \\ndoes not qualify for this year’s Top 5 AI and \\nML Products list. But its accelerated growth \\nwith the Databricks Lakehouse is worth \\nhighlighting, as it speaks volumes about the \\ncurrent state of the industry.\\nThe LangChain and Databrick integration \\nlaunched at the end of April 2023. In just \\nthree months, LangChain became the third \\nmost popular AI and ML product  with our \\ncustomers, following Plotly and Dash and \\nHugging Face. \\nThis data point supports a very clear \\nmessage: enterprises want to use generative \\nAI within their businesses now.Rising \\nStar\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='7bbb6e8e-e0c6-4bd6-8f8c-9ad4381c7f24', embedding=None, metadata={'page_label': '18', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI18\\n FASTEST-GROWING DATA AND AI PRODUCTS \\n0% 50% 100% 150% 200%\\nYear-Over-Year Growth by Number of Customersdbt 206%\\n181%\\n174%\\n152%\\n145%\\n141%\\n110%\\n101%\\n100%\\n95%Esri\\nLooker\\nLyticsHugging FaceFivetran\\nInformatica\\nQlik Data Integration\\nGreat Expectations\\nKepler.gl ', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='812d5326-20ea-4c18-b775-28a252209dfe', embedding=None, metadata={'page_label': '19', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI19\\nDBT IS THE FASTEST-GROWING DATA  \\nAND AI PRODUCT OF 2023\\nThe data ecosystem is undergoing a major transition, \\nand selecting the right products is critical for companies \\nlooking to take advantage of the newest innovations. \\nBecause the Databricks Lakehouse is used broadly across \\nthis ecosystem, it provides unique insights into how \\ncustomers adopt hundreds of data products and services.\\nWe discovered that as companies move quickly to \\ndevelop more advanced use cases, they are investing \\nin newer products that produce trusted data sets for \\nreporting, ML modeling and operational workflows. Hence, \\nwe see the rapid rise of data integration products. dbt, a \\ndata transformation tool, and Fivetran, which automates \\ndata pipelines, are our two fastest-growing data and AI \\nproducts. This suggests a new era of the data integration \\nmarket, with challenger tools making headway as \\ncompanies shift to prioritize DS/ML initiatives. With Great \\nExpectations from Superconductive in the ninth spot,  \\na full 50% of our fastest-growing products represent  \\nthe data integration category.\\nSTATE OF DATA + AI', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='358fed1a-da78-442b-ad2e-515c364e99c0', embedding=None, metadata={'page_label': '20', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20   \\nNote: In this chart, we count the number of customers deploying one or more data and AI products in each category. These four \\ncategories do not encompass all products. Databricks products, such as Unity Catalog, are not included in this data. GROWTH OF DATA AND AI MARKETS \\nFeb \\n2022Mar Apr May June July Aug Sept Oct Nov Dec Jan \\n2023Number of CustomersBusiness Intelligence\\nData Governance  \\n& Security\\nData Science &  \\nMachine Learning\\nData Integration', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c440dcc6-70b5-4fd9-a999-e1f28ecacf8a', embedding=None, metadata={'page_label': '21', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI21\\nTo understand how organizations are prioritizing their \\ndata initiatives, we aggregated all data and AI products on \\nthe Databricks Lakehouse and categorized them into four \\ncore markets: BI, data governance and security, DS/ML, \\nand data integration. Our data set confirms that BI tools \\nare more widely adopted across organizations relative to \\nmore nascent categories — and they continue to grow, \\nwith a 66% YoY increase in adoption. This aligns with the \\nbroader trend of more organizations performing data \\nwarehousing on a Lakehouse, covered in the next section, \\nViews from the Lakehouse .   Data and AI markets: business intelligence is  \\nstandard, organizations invest in their machine  \\nlearning foundation\\nWhile BI is often where organizations start their data \\njourney, companies are increasingly looking at more \\nadvanced data and AI use cases.  \\nDEMAND FOR DATA INTEGRATION PRODUCTS  \\nIS GROWING FAST\\nWe see the fastest growth in the data integration market. \\nThese tools enable a company to integrate vast amounts \\nof upstream and downstream data in one consolidated \\nview. Data integration products ensure that all BI and DS/\\nML initiatives are built on a solid foundation. \\nWhile it’s easier for smaller markets to experience \\nfaster growth, at 117% YoY increased adoption, the data \\nintegration market is growing substantially faster than BI. \\nThis trend dovetails with the rapid growth of ML adoption \\nwe see across the Lakehouse, covered in the DS/ML  \\nsection of the report.  Data integration is the \\nfastest-growing market,  \\nwith 117% YoY growth', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d01166f7-7a6d-40c4-8a23-4e77aa75902f', embedding=None, metadata={'page_label': '22', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Views from \\nthe Lakehouse\\nMIGRATION AND DATA  \\nFORMAT TRENDS \\nData migration is a major undertaking: it can be risky, \\nexpensive and delay companies’ timelines. It’s not a \\ntask to jump into lightly. As organizations run into the \\nlimitations, scalability challenges and the cost burden  \\nof legacy data platforms, they are increasingly likely  \\nto migrate to a new type of architecture.\\nSTATE OF DATA + AI 22', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='80405596-785d-4b00-bf5e-897ca35deb31', embedding=None, metadata={'page_label': '23', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='23 SOURCE OF NEW CUSTOMER   \\n MIGRATIONS TO DATABRICKS Migration trends:  \\nthe best data warehouse  \\nis a Lakehouse\\nThe Lakehouse Platform is an attractive \\nalternative to traditional data warehouses \\nbecause it supports advanced use cases and  \\nDS/ML, allowing organizations to boost their \\noverall data strategy. As evidenced by the most \\npopular data and AI products, with BI and data \\nintegration tools at the top, organizations are \\nincreasingly using the data lakehouse for data \\nwarehousing. To better understand which legacy \\nplatforms organizations are moving away from,  \\nwe look at the migrations of new customers  \\nto Databricks.\\nAn interesting takeaway is that roughly half of the  \\ncompanies moving to the Lakehouse are coming \\nfrom data warehouses. This includes the 22%  \\nthat are moving from cloud data warehouses.  \\nIt also demonstrates a growing focus on running \\ndata warehousing workloads on a Lakehouse  \\nand unifying data platforms to reduce cost. \\nSTATE OF DATA + AI 23\\nOn-Prem Data WarehouseHadoopMultiple\\nMigrations\\nCloud Data Warehouse27%\\n22%39%12%', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='20634ad6-0961-42f7-b74f-56b4605615e7', embedding=None, metadata={'page_label': '24', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI 24Rising tides: the volume  \\nof data in Delta Lake  \\nhas grown 304% YoY\\nAs the volume of data explodes , an increasingly \\nlarge proportion is in the form of semi-structured \\nand unstructured data. Previously, organizations  \\nhad to manage multiple different platforms for  \\ntheir structured, unstructured and semi-structured \\ndata, which caused unnecessary complexity and \\nhigh costs. The Lakehouse solves this problem by \\nproviding a unified platform for all data types  \\nand formats.\\nDelta Lake is the foundation of the Databricks \\nLakehouse. The Delta Lake format encompasses \\nstructured, unstructured and semi-structured  \\ndata. Use has surged over the past 2 years.  \\nWhen compared to the steady, flat or declining \\ngrowth in other storage formats (e.g., text, JSON  \\nand CSV), our data shows that a growing number  \\nof organizations are turning to Delta Lake to \\nmanage their data. In June 2022, Delta Lake \\nsurpassed Parquet as the most popular data lake \\nsource, reaching 304% YoY growth. VOLUME OF DATA MANAGED,  \\n BY STORAGE FORMAT \\nJan \\n2019 Jan \\n2020 Jan \\n2021 Jan \\n2022 Jan \\n2023 Volume of Data\\n24Delta\\nParquetText \\nORCCSV \\nJSONAvro', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='63c71fac-fa6e-4699-8c5a-8e6cd24e1f47', embedding=None, metadata={'page_label': '25', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI25\\nData warehousing grows,  \\nwith emphasis on serverless\\nOver the past 2 years, companies have vastly increased their usage  \\nof data warehousing on the Lakehouse Platform. This is especially \\ndemonstrated by use of Databricks SQL  — the serverless data \\nwarehouse on the Lakehouse — which shows 144% YoY growth.  \\nThis suggests that organizations are increasingly ditching traditional \\ndata warehouses and are able to perform all their BI and analytics  \\non a Lakehouse.\\n \\n DATA WAREHOUSING  \\n ON LAKEHOUSE WITH  \\n DATABRICKS SQL \\nNote: There is a spike in October 2021 \\nas a result of the ungated preview \\nlaunch of Databricks SQL, followed by \\nGeneral Availability in December 2021. \\nData consistently dips in the last \\nweek of December due to seasonality.\\nData  \\nWarehouseLakehouse \\nPlatform\\nJan \\n2021 July \\n2021 Jan \\n2022 July \\n2022 Jan \\n2023 Number of Customers\\nSTATE OF DATA + AI 25', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='ed15c6bf-1853-4a1e-a03f-d4e87ba7c3cf', embedding=None, metadata={'page_label': '26', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI26\\nCONCLUSION\\nGeneration AI\\nWe’re excited that companies are progressing into more \\nadvanced ML and AI use cases, and the modern data \\nand AI stack is evolving to keep up. Along with the rapid \\ngrowth of data integration tools (including our fastest \\ngrowing, dbt), we’re seeing the rapid rise of NLP and LLM \\nusage in our own data set, and there’s no doubt that the \\nnext few years will see an explosion in these technologies. \\nIt’s never been more clear: the companies that harness \\nthe power of DS/ML will lead the next generation of data.\\nSTATE OF DATA + AI 26', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='19ec681b-0ebc-41fa-93ee-ec8a029c270b', embedding=None, metadata={'page_label': '27', 'file_name': 'databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_path': '..\\\\data\\\\databricks-state-of-data-report-010524-v9-FINAL.pdf', 'file_type': 'application/pdf', 'file_size': 772429, 'creation_date': '2024-09-08', 'last_modified_date': '2024-03-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='STATE OF DATA + AI27\\nAbout Databricks\\nDatabricks is the data and AI company. More than 9,000 \\norganizations worldwide — including Comcast, Condé Nast and \\nover 50% of the Fortune 500 — rely on the Databricks Lakehouse \\nPlatform to unify their data, analytics and AI. Databricks is \\nheadquartered in San Francisco, with offices around the globe. \\nFounded by the original creators of Apache Spark™, Delta Lake  \\nand MLflow, Databricks is on a mission to help data teams solve  \\nthe world’s toughest problems. To learn more, follow Databricks  \\non Twitter , LinkedIn  and Instagram .\\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation  | Terms of UseDISCOVER LAKEHOUSE', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that the paper recommends a data lakehouse approach for handling ETL (Extract, Transform, Load) pipelines.\n",
      "\n",
      "Here are some key takeaways:\n",
      "\n",
      "1. **Migrating to a Lakehouse**: The paper suggests that companies are increasingly moving away from traditional data warehouses and migrating to a lakehouse platform, which supports advanced use cases and DS/ML.\n",
      "2. **Unified Data Platform**: The report mentions the importance of unifying data platforms to reduce costs, suggesting that a lakehouse approach can help streamline ETL pipelines by consolidating multiple data sources into one platform.\n",
      "3. **Data Integration**: The paper highlights the growth in demand for data integration products, which enable companies to integrate vast amounts of upstream and downstream data in one consolidated view.\n",
      "\n",
      "Considering these points, I would recommend the following best practices for handling ETL pipelines:\n",
      "\n",
      "1. **Migrate to a Lakehouse Platform**: Consider moving your ETL pipeline to a lakehouse platform, which can provide a unified data environment and support advanced use cases.\n",
      "2. **Centralize Data Integration**: Implement a centralized data integration strategy that allows you to integrate multiple data sources into one consolidated view.\n",
      "3. **Use Cloud-Native Tools**: Leverage cloud-native tools and services for ETL pipeline management, as they often offer scalable, secure, and cost-effective solutions.\n",
      "4. **Monitor and Optimize Pipeline Performance**: Continuously monitor your ETL pipeline's performance and optimize it regularly to ensure efficient data processing and minimal latency.\n",
      "\n",
      "These recommendations are based on the context provided, but I would be happy to discuss further or provide more specific guidance if you'd like!"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "\n",
    "\n",
    "for doc in documents:\n",
    "    fp = doc.metadata['file_path']\n",
    "    doc.metadata['file_path'] = os.path.relpath(fp, cwd)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"context\",\n",
    "    memory = memory,\n",
    "    system_prompt = (\n",
    "        \"You are my helpful assitant\"\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = chat_engine.stream_chat(\"Best aproach to handle ETL pipelines according to the paper\")\n",
    "\n",
    "for token in stream.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file path is: ..\\data\\databricks-state-of-data-report-010524-v9-FINAL.pdf"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader(\"../data\").load_data()\n",
    "cwd = cwd = os.getcwd()\n",
    "\n",
    "for doc in documents:\n",
    "    fp = doc.metadata['file_path']\n",
    "    doc.metadata['file_path'] = os.path.relpath(fp, cwd)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode = \"context\",\n",
    "    memory = memory,\n",
    "    system_prompt = (\n",
    "        \"You are my helpful assitant\"\n",
    "    )\n",
    ")\n",
    "\n",
    "stream = chat_engine.stream_chat(\"Retrieve the filepath\")\n",
    "\n",
    "for token in stream.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Carlos\\miniconda3\\envs\\rag-chat\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda found, falling back to cpu\n",
      "\n",
      "        Context:\n",
      "\n",
      "        User input: Hello\n",
      "        \n",
      "\"HELLO\"\n",
      "Based on the context, I can help you with a few things.\n",
      "\n",
      "Since we have a conversation ID created by `ss.create_conversation('test', chat_history)`, it seems like we're working with a conversation API. \n",
      "\n",
      "If you'd like to continue the conversation and add another message to the chat history, you could do something like this:\n",
      "\n",
      "```python\n",
      "new_message = {'role': 'user', 'content': \"How are you?\"}\n",
      "ss.add_message(conversation_id, new_message)\n",
      "```\n",
      "\n",
      "This would add a new message to the conversation with the role of 'user' and content 'How are you?'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the parent directory (my_project) to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from models import *\n",
    "slm = SLM(model_name='llama3.1')\n",
    "conversation = [{'role': 'user', 'content': \"Hello\"}]\n",
    "try:\n",
    "    stream = slm.chat(chat_history=conversation, data_folder=os.getcwd(),is_stream=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(slm.create_title(\"Hello\"))\n",
    "out=\"\"\n",
    "for chunk in stream.response_gen:\n",
    "    print(chunk, end=\"\")\n",
    "    out+= chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
